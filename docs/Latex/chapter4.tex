% !TEX root = master.tex
\chapter{Abschluss} \label{chapter:4}
% Bewertung und Einordnung der Ergebnisse, Limitierungen der Methoden (z.B. aufgrund begrenzter Computing-Ressourcen), mögliche Erweiterungen.

Die in dieser Gruppenarbeit angewendeten und untersuchten Verfahren haben gezeigt, dass \ac{RL}-Methoden erfolgreich eingesetzt werden können, um in einer simulierten Fußball-Umgebung, in der ein Spieler das Ziel hat, ein Tor zu schießen und auch kompetitiv gegen andere Spieler antreten kann, einen \ac{RL}-Agenten zu trainieren. Besonders der \ac{PPO}-Algorithmus erwies sich als erfolgreich, und auch die Anwendung von \enquote{competitive self-play} im \ac{MARL}-Kontext zeigt erste Potenziale auf.

Nichtsdestotrotz muss angemerkt werden, dass das Verhalten der Agenten im Vergleich zu einem echten Fußballspiel oder auch zu anderen, für die \enquote{Soccer-Twos}-Umgebung veröffentlichungen Implementierungen \cite{todo}, noch zu wünschen übrig lässt. Die Anwendung der erlernten \ac{RL}-Verfahren und das Training der Agenten war in dieser Gruppenarbeit aufgrund der sehr komplexen Umgebung, mit einer Vielzahl an Beobachtungen und der zusätzlichen Schwierigkeit des Multi-Agenten-Szenarios, eine große Herausforderung. Auch die verfügbaren Trainingsressourcen haben den Erfolg der Ergebnisse eingeschränkt.

Auch wenn das Ergebnis dieser Gruppenarbeit somit vom Wunschszenario eines perfekten Fußballteams aus \ac{RL}-Agenten etwas entfernt ist, wurden fortschrittliche \ac{RL}-Methoden implementiert und eine einblicksreiche Auswertung mit einem Vergleich der unterschiedlichen Verfahren erstellt. Da vor allem die Vereinfachung auf eine Single-Agent-Umgebung und die Anwendung des \ac{PPO}-Algorithmus erste, durchaus zufriedenstellende, Ergebnisse erbringt, zeigt die Ausarbeitung Potenzial für mögliche Erweiterungen auf. So könnte zum einen das Training dieses bestehenden erfolgreichen Single-Agent-RL-Verfahrens ausgeweitet und durch erhöhte Computing-Ressourcen verbessert werden. Zum anderen ist es auch weiter denkbar, das Multi-Agenten-Szenario tiefer zu betrachten und komplexere Verfahren wie MADDPG oder ?? zu untersuchen.