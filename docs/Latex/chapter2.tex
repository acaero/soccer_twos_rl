% !TEX root = master.tex
\chapter{Methoden} \label{chapter:2}
%Übersicht der angewandten Methoden, Beschreibung einer Baseline-Methode.

Zum lösen des \enquote{Soccer Twos} Problem wurden im Rahmen dieser Gruppenarbeit verschiedene, unterschiedlich komplexe, Methoden angewendet. Zunächst wurde eine einfach Baseline entwickelt mit der die späteren \ac{RL}-Algorithmen verglichen werden können. Im Folgenden wurde als erstes ein alleiniger \ac{RL}-Agent trainiert, mit dem Ziel ein Tor zu erziehlen. Anschließend wurde versucht dies nun auf das \ac{MARL}-Problem mit dem Training mehrerer Agents zu erweitern.

\section{Baseline}

Um eine adäquate Vergleichbarkeit und Bewertung der implementierten \ac{RL}-Verfahren zu ermöglichen, wurde zunächst eine einfache Baseline-Methode für das \enquote{Soccer Twos}-Problem entworfen und implementiert. Diese Baseline beruht auf den zwei Verhaltensweisen, \enquote{Verteidigungsmodus} und \enquote{Angriffsmodus}, die auf Basis der von der Umgebung bereitgestellten Informationen durchgeführt werden.

Der \enquote{Verteidigungsmodus} wird ausgelöst, sobald sich der Ball auf der Heimseite des Spielfelds befindet und somit die Gefahr für ein Gegentor besteht. Die Agents versuchen in diesem Fall, das eigene Tor zu verteidigen, indem sie sich zwischen das Tor und den Ball bewegen. Dies geschieht, indem sie sich zunächst zum Mittelpunkt zwischen dem Ball und dem Tor drehen und anschließend vorwärts zu diesem Mittelpunkt laufen. Da dies wiederholt ausgeführt wird, bewegen sich die Agents somit zwischen den Ball und das Tor und wehren Schüsse auf das eigene Tor ab.
Der \enquote{Angriffsmodus} wird ausgeführt, sobald der Ball auf die gegnerische Spielfeldseite gelangt und somit ein Angriff auf das gegnerische Tor stattfinden kann. Die Agents drehen sich nun so lange, bis der Winkel zwischen Spieler und Ball kleiner als 30 Grad ist, und laufen dann auf den Ball zu. Befindet sich der Ball jedoch hinter dem Spieler wird die Bewegung angepasst, sodass ein Eigentor verhindert werden kann. Die Idee des \enquote{Angriffsmodus} ist es somit, dass ein Agent alles darauf setzt, einen Schuss durchzuführen und somit ein Tor zu erzielen.

Diese simple Baseline, beruhend auf festen Regeln, ermöglicht ein einfaches Fußballspiel zwischen den Agents, welches nicht besonders komplex und effizient ist, aber ein vereinfachtes Spielgeschehen mit gelegentlichen erfolgreichen Torschüssen oder Abwehraktionen erlaubt. Das Ziel der im Folgenden entwickelten \ac{RL}-Verfahren ist es nun, diese einfache Spielweise zu übertreffen.


\section{Single Agent RL Algorithmen}

Zum Einstieg wurden für das \enquote{Soccer Twos} Problem zunächst einige grundlegende \ac{RL}-Algorithmen angewendet, die sich jedoch nur auf einen Agent beschränken. Es wurde somit eine Vereinfachung vorgenommen und nicht direkt das deutlich komplexere \ac{MARL} Problem, bei welchem mehrere Agents trainiert werden müssen, betrachtet, um zu Beginn die allgemeine Machbarkeit und Komplexität des Trainingsprozesses zu untersuchen. Im Rahmen dieser Vereinfachung werden weiter vier Agents und ein Ball zu Start des Spiels zufällig auf dem Feld platziert, jedoch kann sich nur einer dieser Agents bewegen und wird trainiert. 

Insgesamt wurden zwei Off-Policy- und zwei On-Policy-Methoden für das vereinfachte, auf einen Agent beschränkte Problem angewendet. 
Bei Off-Policy-Verfahren wird die Policy, die der Agent erlernt, unabhängig von jener behandelt, die das Verhalten während des Lernprozesses bestimmt. Das bedeutet, dass die gesammelten Erfahrungen nicht nur von der aktuellen Policy des Agenten abhängen, sondern auch von einer anderen Policy stammen können.
Bei On-Policy-Methoden hingegen erlernt ein Agent die gleiche Policy, die auch während des Lernprozesses verwendet wird, um Aktionen zu bestimmen. Der Agent bewertet und optimiert diese Policy direkt anhand der durch diese Policy generierten Erfahrungen.

%DQN
Ein typischer Ansatz zum lösen von \ac{RL}-Problemen ist der \textbf{\ac{DQN}} Algorithms. Es handelt sich hierbei um eine Off-Policy Methode welche das Q-Learning mit tiefen neuronalen Netzwerken kombiniert, um optimale Handlungsstrategien zu erlernen. Der Kern des \ac{DQN} besteht darin, eine Q-Funktion zu approximieren, die den erwarteten zukünftigen Belohnungswert für eine gegebene Aktion in einem bestimmten Zustand angibt. %hier könnte mathematische funktion aber paper sonst schnell zu lang
Durch kontinuierliches Training und Anpassung der Gewichte des neuronalen Netzwerks kann der Agent somit lernen, optimale Aktionen zu wählen, die langfristig zu höheren Belohnungen führen \cite{mnih2013}. 
%Eine Erweiterung dieses Verfahrens stellt der \ac{DDQN} Algorithmus dar.  Dieser soll die Überschätzung von Q-Werten verringern, indem zwei separate Netzwerke verwendet werden. Eines dieser Netze übernimmt die Auswahl der Aktionen und das andere ist für die Bewertung dieser zuständig. Diese Trennung ermöglicht eine stabilere und genauere Schätzung der Q-Werte, was zu einem robusteren Lernprozess führt.

%PG
Ein ebenfalls verbreitete Off-Policy Methode ist der \ac{PG} Algorithmus, welcher sowohl für kontinuierlichen als auch diskrete Aktionsräume geeignet ist. Bei diesem Verfahren wird direkt eine Policy-Funktion, in Form eines Neuronalen Netzes, erlernt, die angibt, welche Aktion in einem gegebenen Zustand ausgeführt werden soll.
Eine Erweiterung dieser Methode stellt der \ac{DPG} und auch \textbf{\ac{DDPG}} Algorithmus dar. Diese verwenden zwei Netzwerke: ein Actor-Netzwerk, das die Policy darstellt und Aktionen auswählt, und ein Critic-Netzwerk, das den Q-Wert der ausgewählten Aktionen schätzt. Es wird somit die Stabilität und Effizienz des Lernens verbessert und bietet vor allem bei komplexeren Aufgaben in kontinuierlichen Aktionsräumen Vorteile. 
%DDPG noch detailierter?
%ggf. nochmal darauf eingehen das wir das trotz diskreten Aktionsraum nutzen und warum

%PPO
Ein weiterer bedeutender Ansatz in diesem Bereich ist die \textbf{\ac{PPO}} Methode. \ac{PPO} ist ein On-Policy Algorithms, gehört zur Familie der \ac{PG} Methoden und zielt darauf ab, die Instabilitäten und hohen Varianzen traditioneller \ac{PG} Algorithmen zu reduzieren. Dies wird durch die Einführung eines speziellen Clipping-Mechanismus erreicht, der die Updates der Policy innerhalb eines bestimmten Bereichs hält. Dieser Mechanismus stellt sicher, dass die Änderungen an der Policy während des Trainings nicht zu drastisch sind, wodurch ein stabilerer und effizienterer Lernprozess gewährleistet wird \cite{SchulmanWDRK17}.

%A2C
Ein weiterere On-Policy Methode ist der \textbf{\ac{A2C}} Algorithmus.
\ac{A2C} kombiniert Elemente der Actor-Critic-Architektur mit Vorteilsschätzungen (Advantage-Funktion). Der Kern der \ac{A2C} Methode besteht somit wie bei den vorherigen Methoden ebenfalls aus einem Policy- und Critic-Netzwerk. Zusätzlich wird jedoch noch eine Advantage-Funktion erlernt, welche den Vorteil einer Aktion in einem bestimmten Zustand im Vergleich zum durchschnittlichen erwarteten Wert aller möglichen Aktionen in diesem Zustand, bestimmt. Durch die Berechnung des Advantage-Werts kann der Algorithmus genauer die Unterschiede zwischen guten und schlechten Aktionen erfassen, was zu effizienteren Policy-Updates führt \cite{MnihBMGLHSK16}.

%SAC
%Ein weiterere Methode ist der \ac{SAC} Algorithms. \ac{SAC} kombiniert Elemente der Actor-Critic-Architektur mit einer Entropiemaximierung, um eine explorativere und robustere Policy zu erlernen. Der Kern der \ac{SAC} Methode besteht somit ebenfalls darin, sowohl ein Policy-Netzwerk (Actor) als auch ein Q-Netzwerk (Critic) zu erlernen. Der wesentliche Unterschied zu anderen Methoden liegt jedoch in der Maximierung der Entropie der Policy. Dies bedeutet, dass der Agent nicht nur darauf abzielt, die Belohnung zu maximieren, sondern auch die Unsicherheit seiner Entscheidungen zu berücksichtigen und somit eine vielfältigere Aktionsauswahl zu ermöglichen. Diese Entropie-Maximierung fördert eine bessere Exploration und verhindert das Verharren in suboptimalen Policys \cite{todo}. 

% NN Struktur
Da die Ansteuerung der Agents, wie zuvor beschrieben, über einen dreielementigen Aktionsvektor mit drei möglichen Werten für jedes Element erfolgt, wurde die Struktur der bei den \ac{RL}-Algorithmen verwendeten neuronalen Netze entsprechend angepasst. Die Ausgabeschicht des Netzwerkes besteht aus neun Neuronen, die in drei Gruppen, welche jeweils eine Bewegungsdimension darstellen, aufgeteilt sind. Innerhalb jeder dieser Gruppen steht je ein Neuron für keine Ansteuerung, die erste mögliche Aktion oder die zweite mögliche Aktion. Die Auswahl der letztendlich durchgeführten Aktion für dieses Element des Aktionsvektors erfolgt durch die Wahl des Maximalwertes innerhalb dieser Gruppe.

%Da die Ansteuerung der Agents, wie zuvor beschrieben, über einen dreielementigen Aktionsvektor mit drei möglichen Werten für jedes Element erfolgt, wurde die Struktur der bei den \ac{RL}-Algorithmen verwendeten neuronalen Netze entsprechend angepasst. Die Ausgabeschicht des Netzwerkes besteht aus neun Neuronen, die in drei Gruppen, welche jeweils eine Bewegungsdimension darstellen, aufgeteilt sind. Innerhalb jeder dieser Gruppen wird ein Softmax angewendet und je ein Neuron steht für keine Ansteuerung, die erste mögliche Aktion. Die Auswahl der letztendlich durchgeführten Aktion für dieses Element des Aktionsvektors erfolgt durch die Wahl des Maximalwertes innerhalb dieser Gruppe.


%Sparse Rewards
Bereits beim Training dieser ersten Algorithmen wurden einige Auffälligkeiten beobachtet. Besonders die Tatschache das in der \enquote{Soccer-Twos}-Umgebung nur relativ selten ein Tor und somit eine Belohnung für den Agenten erziehlt wird führte zu einem sehr langsamen Lernprozess und schlechten Ergebnissen. Da es sich hier somit um eine sogennante \enquote{sparse reward} Umgebung handelt, in welcher die Belohnungen selten und unregelmäßig auftreten, wurde die Behlohnungsfunktion der Agenten angepasst. Neben einer Belohung für ein erzieltes Tor erhalten die Agenten nun fortlaufend einen Belohnung welche auf ihrem Abstand zum Ball baisert. Diese neue Behlohung nimmt \(1\) an, wenn der Abstand zum Ball minimal ist und \(0\) wenn der Abstand maximal, also 32 Einheiten, groß ist. Die neue Belohnungsstruktur der Umgebung kann somit mit folgender Formel zusammengefasst werden:

\[
\text{Belohnung} = \begin{cases} 
	1 - \text{akkumulierte Zeitstrafe} & \text{wenn ein Tor erzielt wird} \\
	-1 & \text{wenn ein Gegentor erzielt wird} \\
	\max\left(0, 1 - \frac{\text{Abstand zum Ball}}{\text{maximaler Abstand}}\right) & \text{sonst}
\end{cases}
\]

Durch diese Anpassung wird der Agent nun angeregt sicher schneller zum Ball zu bewegen und die Wahrscheinlichkeit das er eine sinnvolle Aktion und letztendlich ein Tor erziehlt steigt.

%Stable Baseline
Eine weitere wichtige Entscheidung, die im Laufe des Entwicklungsprozesses getroffen wurde, war die Verwendung der \ac{SB3} Bibliothek \cite{stable-baselines3} für die Implementierung der \ac{PPO} und \ac{A2C} Methoden. Es handelt sich hierbei um eine Open-Scource-Bibliothek welche robuste und effiziente Implementierungen einer Vielzahl an moderneren \ac{RL}-Algorithmen bietet.
Zu Beginn wurden die verschiedenen Verfahren zwar eigenständig implementiert, jedoch stießen diese aufgrund der hohen Komplexität schnell an ihre Grenzen. Durch die Nutzung von \ac{SB3} konnte eine korrekte Implementierung sichergestellt werden. Zudem ermöglicht die Bibliothek den Einsatz fortschrittlicher Optimierungsverfahren und unterstützt Multiprocessing während des Trainings. Dies führte zu einer erheblichen Verbesserung der Trainingszeit und der erzielten Ergebnisse.

\section{Multi Agent Methode}

 Aufbauend auf den bei der Implementierung der vorherigen Methoden gewonnenen Erkenntnissen wurde im Folgenden das \ac{MARL}-Problem betrachtet. Da es sich hierbei um ein großes Themenfeld mit vielen, teils sehr komplexen Methoden handelt und bereits beim Training in einfacheren Single-Agent-Umgebungen einige Herausforderungen auftraten, wurden die Untersuchungen im \ac{MARL}-Bereich auf die Anwendung eines \enquote{competitive self-plays} mit dem \ac{PPO}-Algorithmus beschränkt.
 
 Bei \enquote{competitive self-play} spielt und lernt ein Agent, indem er gegen sich selbst oder eine alternative Version seiner selbst antritt. Im Kontext des \enquote{Soccer-Twos}-Problems wurde eine Population an Agenten erzeugt, die in anfangs zufällig zugeordneten Spielen gegeneinander antreten. Basierend auf diesen Spielen erhalten die Agenten ein Update zu ihrem ELO-Rating, einer Bewertung, die ihre relative Spielstärke widerspiegelt, und sammeln währenddessen individuell Erfahrungen. Nach ihren Spielen werden diese Erfahrungen genutzt, um ihre jeweilige Policy zu aktualisieren. Dieses Verfahren wird wiederholt fortgesetzt, wobei jedoch nun die Zuordnung der Spielgegner aufgrund der ELO-Ratings getroffen wird. Hierdurch kann sichergestellt werden, dass ein Agent gegen einen Gegner auf ähnlichem Niveau antritt und so durch die angemessene Herausforderung des Spiels in der Lage ist, seine eigenen Fähigkeiten iterativ zu verbessern.