{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soccer_twos\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.logger import CustomLogger\n",
    "from src.utils import shape_rewards\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size=336, action_size=9):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.dense_256 = nn.Linear(input_size, 256)\n",
    "        self.mu_dense_128 = nn.Linear(256, 128)\n",
    "        self.mu_dense_64 = nn.Linear(128, 64)\n",
    "        self.mu_dense_32 = nn.Linear(64, 32)\n",
    "        self.action_out = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.dense_256(state))\n",
    "        x = F.relu(self.mu_dense_128(x))\n",
    "        x = F.relu(self.mu_dense_64(x))\n",
    "        x = F.relu(self.mu_dense_32(x))\n",
    "        action_logits = self.action_out(x)\n",
    "        return action_logits\n",
    "\n",
    "\n",
    "class PGAgent:\n",
    "    def __init__(self, state_size=336, action_size=9, learning_rate=0.0003):\n",
    "        self.policy = PolicyNetwork(state_size, action_size)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy.to(self.device)\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = 1\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action_logits = self.policy(state)\n",
    "            action_probs = F.softmax(action_logits, dim=-1)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        return action\n",
    "\n",
    "    def update(self, states, actions, rewards):\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "\n",
    "        # Normalize rewards\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "\n",
    "        # Calculate log probabilities\n",
    "        action_logits = self.policy(states)\n",
    "        log_probs = F.log_softmax(action_logits, dim=-1)\n",
    "        selected_log_probs = log_probs[torch.arange(len(actions)), actions]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = -(selected_log_probs * rewards).mean()\n",
    "\n",
    "        # Update policy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.policy.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.policy.load_state_dict(torch.load(filename))\n",
    "\n",
    "\n",
    "def action_to_env_format(action):\n",
    "    \"\"\"Convert single integer action to the format expected by the environment.\"\"\"\n",
    "    env_action = [0, 0, 0]\n",
    "    env_action[action // 3] = (action % 3) - 1\n",
    "    return env_action\n",
    "\n",
    "\n",
    "def train_pg(n_games, n_agents):\n",
    "    env = soccer_twos.make()\n",
    "    pg_agent = PGAgent()\n",
    "    logger = CustomLogger(\"pg_agent\", \"pg_simple\")\n",
    "    for i in range(n_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        scores = {}\n",
    "        while not done:\n",
    "            actions = {}\n",
    "            for j in range(4):\n",
    "                if j < n_agents:\n",
    "                    action = pg_agent.act(obs[j])\n",
    "                    actions[j] = action_to_env_format(action)\n",
    "                else:\n",
    "                    actions[j] = [0, 0, 0]  # No action for non-agent players\n",
    "\n",
    "            next_obs, reward, done, info = env.step(actions)\n",
    "            done = done[\"__all__\"]\n",
    "\n",
    "            for i in range(4):\n",
    "                scores[i] = shape_rewards(info, i)\n",
    "            # Only store data for the PG agent (agent 0)\n",
    "            episode_states.append(obs[0])\n",
    "            episode_actions.append(action)  # Store the action index\n",
    "            episode_rewards.append(scores[0])\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # Update the agent after each episode\n",
    "        pg_agent.update(\n",
    "            np.array(episode_states),\n",
    "            np.array(episode_actions),\n",
    "            np.array(episode_rewards),\n",
    "        )\n",
    "        logger.write_logs_and_tensorboard(\n",
    "            i, scores, next_obs, reward, done, info, actions, pg_agent\n",
    "        )\n",
    "        # Here you might want to add logging or printing of episode results\n",
    "        print(f\"Episode {i+1} finished. Total reward: {sum(episode_rewards)}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_pg(n_games=1000, n_agents=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
